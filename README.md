# ğŸ“š AI Research Assistant

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![LangGraph](https://img.shields.io/badge/built%20with-LangGraph-ff69b4.svg)](https://langchain.com/)

An intelligent research assistant that automates the process of academic research, from topic exploration to comprehensive report generation using cutting-edge AI technologies.

## ğŸš€ Features

- **Smart Topic Analysis** - Get detailed explanations and related concepts for any research topic
- **Automated Paper Discovery** - Find relevant academic papers with smart filtering
- **AI-Powered Summarization** - Extract key insights from multiple research papers
- **Comprehensive Reporting** - Generate well-structured research reports with proper citations
- **Local Processing** - Run everything on your machine with Ollama's local LLMs

## ğŸ—ï¸ Tech Stack

### Core Technologies

| Technology | Purpose |
|------------|---------|
| **Python 3.8+** | Core programming language |
| **LangGraph** | Workflow orchestration and state management |
| **Ollama** | Local LLM inference (supports llama2, mistral, etc.) |
| **scholarly** | Academic paper search and metadata extraction |
| **PyPDF2** | PDF text extraction and processing |

### Key Dependencies

- **langchain**: Framework for building LLM applications
- **pydantic**: Data validation and settings management
- **requests**: HTTP requests for API calls
- **tqdm**: Progress bars for long-running operations
- **python-dotenv**: Environment variable management

## ğŸ› ï¸ Installation

1. **Clone the repository**
   ```bash
   git clone https://github.com/yourusername/research-assistant-ai.git
   cd research-assistant-ai
   ```

2. **Set up a virtual environment**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   cd research_assistant
   pip install -r requirements.txt
   ```

4. **Set up Ollama**
   - Download and install from [ollama.ai](https://ollama.ai/)
   - Start the Ollama server:
     ```bash
     ollama serve
     ```
   - In a new terminal, download a model:
     ```bash
     ollama pull llama2  # or mistral, etc.
     ```

## ğŸš€ Quick Start

```bash
python -m research_assistant.agent_graph --topic "Your Research Topic"
```

### Command Line Arguments

| Argument | Description | Default |
|----------|-------------|---------|
| `--topic` | Research topic (required) | - |
| `--max_papers` | Maximum papers to process | 10 |
| `--min_year` | Minimum publication year | 2020 |
| `--output_dir` | Directory to save outputs | ./notes |

## ğŸ—ï¸ Project Structure

```
research-assistant-ai/
â”œâ”€â”€ research_assistant/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ agent_graph.py         # Main workflow definition
â”‚   â”œâ”€â”€ nodes/                 # Individual processing nodes
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ topic_explainer.py
â”‚   â”‚   â”œâ”€â”€ scholar_search.py
â”‚   â”‚   â”œâ”€â”€ pdf_downloader.py
â”‚   â”‚   â””â”€â”€ ...
â”‚   â””â”€â”€ utils/                 # Helper functions
â”‚       â”œâ”€â”€ __init__.py
â”‚       â”œâ”€â”€ pdf_processor.py
â”‚       â””â”€â”€ ollama_processor.py
â”œâ”€â”€ notes/                     # Generated research notes
â”œâ”€â”€ downloaded_pdfs/           # Downloaded research papers
â”œâ”€â”€ requirements.txt           # Python dependencies
â””â”€â”€ README.md                  # This file
```

## ğŸ”„ Workflow

1. **Topic Analysis**
   - The system first analyzes the provided research topic
   - Generates a detailed explanation and identifies related concepts

2. **Paper Discovery**
   - Searches academic databases for relevant papers
   - Filters results based on relevance and publication year

3. **Content Processing**
   - Downloads available PDFs
   - Extracts and cleans text content
   - Processes multiple papers in parallel

4. **Analysis & Summarization**
   - Uses local LLMs to generate summaries
   - Extracts key insights and methodologies
   - Identifies research gaps and trends

5. **Report Generation**
   - Compiles findings into structured markdown
   - Includes proper citations and references
   - Generates an executive summary

## ğŸ¤– Customization

### Changing the Research Topic
You can modify the research topic in two ways:

1. **Command Line Argument** (Recommended):
   ```bash
   python -m research_assistant.agent_graph --topic "Your Research Topic"
   ```

2. **Direct Code Modification**:
   Edit the `initial_state` dictionary in `agent_graph.py`:
   ```python
   initial_state = {
       "topic": "Your Research Topic Here",  # Change this to your desired topic
       "explanation": None,
       "related_topics": None,
       "search_results": None,
       # ... other fields remain the same
   }
   ```

### Using Different Models
Edit the `.env` file to specify your preferred Ollama model:
```
OLLAMA_MODEL=llama2  # Change to mistral, codellama, etc.
```

### Adding New Features
1. Create a new node in `research_assistant/nodes/`
2. Import and add it to the workflow in `agent_graph.py`
3. Define the edges to connect it with existing nodes

## ğŸ“ Example Output

```markdown
# Research Report: AI in Education

## Executive Summary
[Generated summary of findings...]

## Key Papers
1. **Title of Paper 1**  
   Authors, Year  
   Summary: [Generated summary...]

2. **Title of Paper 2**  
   Authors, Year  
   Summary: [Generated summary...]

## Research Gaps
- [Identified gap 1]
- [Identified gap 2]

## Future Directions
- [Suggested research direction 1]
- [Suggested research direction 2]
```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Built with [LangChain](https://python.langchain.com/)
- Powered by [Ollama](https://ollama.ai/)
- Inspired by the latest research in AI and NLP
